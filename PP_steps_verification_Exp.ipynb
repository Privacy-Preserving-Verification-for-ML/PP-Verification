{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d81fb3-dcfe-4505-9c2a-1a2a710de2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from preprocessing_pipeline import PreprocessingPipeline\n",
    "from explanation_collection import explanation_collection\n",
    "from differential_privacy import DifferentialPrivacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f565223d-8bcd-4c22-acb5-5cdf31ac4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = int(os.environ.get('DATASET_ID'))\n",
    "numerical_columns = os.environ.get('NUMERICAL_COLUMNS')\n",
    "numerical_columns = [item.strip() for item in numerical_columns.split(',')]\n",
    "case_id = int(os.environ.get('CASE_ID'))\n",
    "epsilon = int(os.environ.get('EPSILON'))\n",
    "rep = int(os.environ.get('REPEAT'))\n",
    "\n",
    "path = \"./results_Exp\"\n",
    "\n",
    "# dataset_id = 891\n",
    "# numerical_columns = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "# case_id = 0\n",
    "# epsilon = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4003ca25-a9f8-4e4a-9502-8f44470a0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "uci_dataset = fetch_ucirepo(id=dataset_id)\n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = uci_dataset.data.features \n",
    "y = uci_dataset.data.targets \n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "target_col = y.columns[0]\n",
    "\n",
    "# Select the remaining columns as categorical columns\n",
    "all_columns = set(X.columns)\n",
    "categorical_columns = list(all_columns - set(numerical_columns))\n",
    "\n",
    "pipeline = PreprocessingPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232e445d-9538-41c9-bdf2-f51f1b88253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Dropped: 0 rows removed\n",
      "Percentage of rows dropped due to missing values: 0.00%\n",
      "number of categorical columns '14\n"
     ]
    }
   ],
   "source": [
    "steps_prior = [\n",
    "    'drop_missing_values',\n",
    "\n",
    "    'encode_categorical_variables',\n",
    "]\n",
    "\n",
    "X, y = pipeline.preprocess(data, target_col, numerical_columns, categorical_columns, steps_prior)\n",
    "\n",
    "if dataset_id == 2:\n",
    "    y = np.where((y == 0) | (y == 1), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0c0404-fb17-4889-a103-15cc5a98d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_o, X_q, y_o, y_q = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_o to a pandas Series\n",
    "y_o_series = pd.Series(y_o, name=target_col)\n",
    "y_q_series = pd.Series(y_q, name=target_col)\n",
    "\n",
    "# Reset index of X_o\n",
    "X_o = X_o.reset_index(drop=True)\n",
    "X_q = X_q.reset_index(drop=True)\n",
    "\n",
    "# Combine X and y into a single DataFrame\n",
    "df = pd.concat([X_o, y_o_series], axis=1)\n",
    "    \n",
    "df_q = pd.concat([X_q, y_q_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b93233-92d5-4e2c-934e-becbdf18d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DifferentialPrivacy(epsilon)\n",
    "\n",
    "df_r = df.copy()\n",
    "\n",
    "if epsilon != 0:\n",
    "    df_v = dp.apply_differential_privacy(df, numerical_columns, categorical_columns, round_to_int=True)\n",
    "else:\n",
    "    df_v = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881f4658-a33c-419d-b0dd-7d17a3a9cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combination(case_id):\n",
    "    elements = [1, 2, 3, 4]\n",
    "    all_combinations = []\n",
    "\n",
    "    # Generate all non-empty combinations of the elements\n",
    "    for r in range(4, 0, -1):\n",
    "        all_combinations.extend(combinations(elements, r))\n",
    "\n",
    "    # Map the case_id to the corresponding combination\n",
    "    if 0 <= case_id < len(all_combinations):\n",
    "        return list(all_combinations[case_id])\n",
    "    else:\n",
    "        return \"Invalid case_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f57b95-cbdb-445d-ba8b-c3047ce12d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['drop_all_duplicates', 'handle_outliers', 'scale_numerical_features_only', 'resample_data']\n",
    "\n",
    "steps_in_r = get_combination(case_id)\n",
    "steps_in_v = get_combination(case_id)\n",
    "\n",
    "steps_r = [steps[i-1] for i in steps_in_r]\n",
    "steps_v = [steps[i-1] for i in steps_in_v]\n",
    "\n",
    "steps_qr = None\n",
    "steps_qv = None\n",
    "\n",
    "if 3 in steps_in_r:\n",
    "    steps_qr = [steps[2]]\n",
    "if 3 in steps_in_v:\n",
    "    steps_qv = [steps[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74345506-51a4-4c91-a50e-2beaaf604949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates Dropped: 17359 rows removed\n",
      "Percentage of rows dropped due to duplicates: 8.55%\n",
      "Outliers: 15233 rows removed\n",
      "Percentage of rows dropped due to outliers: 8.21%\n",
      "number of numerical columns '7\n",
      "Mean relative change for each feature:\n",
      " BMI          0.970887\n",
      "GenHlth      0.780531\n",
      "MentHlth     0.513143\n",
      "PhysHlth     0.599856\n",
      "Age          0.935646\n",
      "Education    0.858547\n",
      "Income       0.911513\n",
      "dtype: float64\n",
      "Overall mean relative change: 0.7957321294793795\n",
      "Percentage change in row count due to resampling: 71.44%\n",
      "Duplicates Dropped: 17359 rows removed\n",
      "Percentage of rows dropped due to duplicates: 8.55%\n",
      "Outliers: 15233 rows removed\n",
      "Percentage of rows dropped due to outliers: 8.21%\n",
      "number of numerical columns '7\n",
      "Mean relative change for each feature:\n",
      " BMI          0.970887\n",
      "GenHlth      0.780531\n",
      "MentHlth     0.513143\n",
      "PhysHlth     0.599856\n",
      "Age          0.935646\n",
      "Education    0.858547\n",
      "Income       0.911513\n",
      "dtype: float64\n",
      "Overall mean relative change: 0.7957321294793795\n",
      "Percentage change in row count due to resampling: 71.44%\n",
      "number of numerical columns '7\n",
      "number of numerical columns '7\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing pipeline\n",
    "if steps_r:\n",
    "    X_r, y_r = pipeline.preprocess(df_r, target_col, numerical_columns, categorical_columns, steps_r, train=True)\n",
    "else:\n",
    "    X_r, y_r = df.iloc[:,0:-1], df.iloc[:, -1]\n",
    "\n",
    "if steps_v:\n",
    "    X_v, y_v = pipeline.preprocess(df_v, target_col, numerical_columns, categorical_columns, steps_v, train=True)\n",
    "else:\n",
    "    X_v, y_v = df.iloc[:,0:-1], df.iloc[:, -1]\n",
    "\n",
    "# For querying\n",
    "if steps_qr:\n",
    "    X_qr, y_qr = pipeline.preprocess(df_q, target_col, numerical_columns, categorical_columns, steps_qr, train=False, X_train=X_r)\n",
    "else:\n",
    "    X_qr, y_qr = df_q.iloc[:, 0:-1], df_q.iloc[:, -1]\n",
    "\n",
    "if steps_qv:\n",
    "    X_qv, y_qv = pipeline.preprocess(df_q, target_col, numerical_columns, categorical_columns, steps_qv, train=False, X_train=X_v)\n",
    "else:\n",
    "    X_qv, y_qv = df_q.iloc[:, 0:-1], df_q.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdd286f-71cb-4a54-9384-e2a4a1c5eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Without Preprocessing\n",
      "Accuracy: 0.13790996531062757\n"
     ]
    }
   ],
   "source": [
    "# Resercher Side\n",
    "# Train a Logistic Regression classifier\n",
    "model_r = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_r.fit(X_r, y_r)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_r = model_r.predict(X_qr)\n",
    "\n",
    "print(\"Logistic Regression Without Preprocessing\")\n",
    "print(\"Accuracy:\", accuracy_score(y_qr, y_pred_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f10697-ee00-461a-a359-80d3970e85fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression With Preprocessing\n",
      "Accuracy: 0.13790996531062757\n"
     ]
    }
   ],
   "source": [
    "# Verifier Side\n",
    "# Train a Logistic Regression classifier\n",
    "model_v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_v.fit(X_v, y_v)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_v = model_v.predict(X_qv)\n",
    "\n",
    "print(\"Logistic Regression With Preprocessing\")\n",
    "print(\"Accuracy:\", accuracy_score(y_qv, y_pred_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9edfd8-8f87-4cc5-ac54-cd897c525229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                      | 500/50736 [00:10<17:21, 48.24it/s]\n"
     ]
    }
   ],
   "source": [
    "exp_collect = explanation_collection()\n",
    "\n",
    "explanations_r = exp_collect.get_explanations(X_r, X_qr, model_r)\n",
    "df_explanations_r = pd.DataFrame(explanations_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf110d70-0461-4632-93a5-b52c8cda4bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                      | 500/50736 [00:10<17:19, 48.33it/s]\n"
     ]
    }
   ],
   "source": [
    "explanations_v = exp_collect.get_explanations(X_v, X_qv, model_v)\n",
    "df_explanations_v = pd.DataFrame(explanations_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe4379f-228b-4dc6-a6d2-e781adf0e454",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_explanations_r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m case_id\n\u001b[0;32m----> 2\u001b[0m df_explanations_r\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dataset\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eps\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_resR_case\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df_explanations_v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m case_id\n\u001b[1;32m      5\u001b[0m df_explanations_v\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dataset\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eps\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_resV_case\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "df_explanations_r['case'] = case_id\n",
    "df_explanations_r.to_csv(f'{path}/dataset{dataset_id}_eps{epsilon}_resR_case{case_id}_rep{rep}.csv', index=False)\n",
    "\n",
    "df_explanations_v['case'] = case_id\n",
    "df_explanations_v.to_csv(f'{path}/dataset{dataset_id}_eps{epsilon}_resV_case{case_id}_rep{rep}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277b193-ce02-4e32-8511-0f00f3763e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
